```
Erstelle einen neuen Scraper für untermuenkheim im Landkreis Schwäbisch hall

## Basis-Informationen
- Gemeinde/Stadt: Untermünkheim
- Landkreis: Schwäbisch Hall
- Website-URL (Events): https://api.cross-7.de/public/calendar/1203/events?pageNumber=1&pageSize=150&excludeCategoryIds=130585&excludeCategoryIds=130584&excludeCategoryIds=130586&from=2026-02-10T09%3A00%3A00.000Z
- PLZ + Ort (für Geocoding): 74547 Untermünkheim




## HTML-Struktur
Der Scraper soll die API Rest-Schnittstelle verwenden. Für den "get-Paremeter" in der url soll der "from" Wert auf den aktuellen Tag gesetzt werden. Da es sich hier um eine api-schnittstelle handelt, und nicht um html scraping, kannst du dich am oehringen.py scraper orientieren. Dieser verwendet dieselbe schnittstelle

Hier ist das beispiel JSON
{
  "items": [
    {
      "fromDate": "2026-02-12",
      "untilDate": "2026-02-12",
      "addresses": [
        {
          "type": "Veranstalter",
          "name": "Seniorenverein Untermünkheim",
          "private": false
        },
        {
          "type": "Veranstaltungsort",
          "name": "Ev. Gemeindehaus",
          "private": false
        }
      ],
      "link": {
        "targetId": 22681756,
        "slug": "/kultur-tourismus/veranstaltungen-vereine/veranstaltungskalender"
      },
      "name": "Seniorennachmittag",
      "created": "2025-11-26T14:25:11.110007Z",
      "teaserPictureUrl": "https://api.cross-7.de/public/files/781296e6-3e74-4445-a39c-4b0bb78f1de2",
      "categoryColor": "#474747",
      "teaserText": "",
      "hasContent": false,
      "categoryNames": [
        {
          "id": 129167,
          "name": "Feiern & Geselligkeit",
          "color": "#474747",
          "discriminator": "CategoryNameDto"
        }
      ],
      "additionalFields": [],
      "discriminator": "CalendarEventTeaserDto"
    }
  ],
  "pageNumber": 1,
  "totalPages": 132,
  "totalCount": 132,
  "hasPreviousPage": false,
  "hasNextPage": true
}

## Projekt-Kontext (für Claude Code)

### Projektstruktur

```
src/
├── scrapers/
│   ├── base.py                          # BaseScraper + ScrapedEvent
│   ├── __init__.py                      # Zentrale Exports aller Scraper
│   └── baden_wuerttemberg/
│       ├── __init__.py                  # Exports für ganz BW
│       ├── hohenlohekreis/
│       │   ├── __init__.py              # Exports für den Landkreis
│       │   ├── mulfingen.py
│       │   ├── kuenzelsau.py
│       │   └── ...
│       ├── schwaebisch_hall/
│       │   ├── __init__.py
│       │   └── crailsheim.py
│       └── main_tauber_kreis/
│           └── __init__.py
├── cli.py                               # CLI mit SCRAPER_REGISTRY
├── debug_scraper.py                     # Debug-Tool mit SCRAPER_REGISTRY
└── api/
    └── routers/
        └── scraper.py                   # API mit SCRAPER_REGISTRY
```

### Dateien die bei einem neuen Scraper angepasst werden müssen (6 Stück)

1. **Scraper-Datei erstellen**: `src/scrapers/baden_wuerttemberg/{landkreis}/{gemeinde}.py`
2. **Landkreis `__init__.py`**: `src/scrapers/baden_wuerttemberg/{landkreis}/__init__.py` - Import + `__all__`
3. **BW `__init__.py`**: `src/scrapers/baden_wuerttemberg/__init__.py` - Import + `__all__`
4. **Scrapers `__init__.py`**: `src/scrapers/__init__.py` - Import + `__all__`
5. **CLI**: `src/cli.py` - Import + `SCRAPER_REGISTRY`
6. **Debug-Tool**: `src/debug_scraper.py` - Import + `SCRAPER_REGISTRY`
7. **API-Router**: `src/api/routers/scraper.py` - Import + `SCRAPER_REGISTRY`

> Falls ein neuer Landkreis angelegt wird, muss auch das Landkreis-Package (`__init__.py`) erstellt werden.

### BaseScraper Interface

Jeder Scraper erbt von `BaseScraper` und muss definieren:

```python
from ...base import BaseScraper, ScrapedEvent

class BeispielScraper(BaseScraper):
    SOURCE_NAME = "Gemeinde Beispiel"           # Anzeigename
    BASE_URL = "https://www.beispiel.de"        # Basis-URL
    EVENTS_URL = "https://www.beispiel.de/..."  # Startseite für Events
    GEOCODE_REGION = "74XXX Beispiel"           # PLZ + Ort für Google Geocoding

    SELECTORS = {
        "event_container": "...",  # CSS-Selektor für Event-Container
        "title": "...",            # Titel
        "date": "...",             # Datum
        "time": "...",             # Uhrzeit (optional)
        "location": "...",         # Veranstaltungsort (optional)
        "url": "...",              # Link zur Detailseite (optional)
    }

    def parse_events(self, soup: BeautifulSoup) -> List[ScrapedEvent]:
        """Muss implementiert werden. Gibt Liste von ScrapedEvent zurück."""
        ...
```



### Wichtige Hinweise

- **`raw_location`** ist der Schlüssel für Location-Matching. Ohne `raw_location` wird kein Location-Eintrag erstellt und das Event erscheint nicht auf der Karte.
- **`external_id`** muss pro Source eindeutig sein. Format-Konvention: `{gemeinde}_{id}` (z.B. `crailsheim_12345`).
- **`location_postal_code`** darf max. 10 Zeichen haben (DB: `VARCHAR(10)`). Falls der Wert "74564 Crailsheim" enthält, muss die PLZ extrahiert werden.
- **Pagination**: Falls die Website paginiert ist, muss `parse_events()` alle Seiten durchlaufen. Verwende `self.fetch_page(url)` für weitere Seiten.
- **Geocoding**: Wird automatisch von `BaseScraper.get_or_create_location()` ausgeführt wenn keine Koordinaten vorhanden sind. Benötigt `GEOCODE_REGION`.
- **Detail-Seiten**: Falls Location-Daten nur auf Detail-Seiten stehen, in `parse_events()` oder einer Helper-Methode `self.fetch_page(detail_url)` aufrufen.

### Verfügbare Helper-Methoden von BaseScraper

```python
self.fetch_page(url)           # Seite laden -> BeautifulSoup (mit Rate Limiting)
self.resolve_url(relative_url) # Relative URL -> Absolute URL
self.location_exists(raw_name) # Prüft ob Location schon in DB existiert (spart Detail-Requests)
```
